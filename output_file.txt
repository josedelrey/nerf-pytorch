# ./train.py

import os
import argparse
import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
from torch.optim.lr_scheduler import ExponentialLR
from torch.utils.tensorboard import SummaryWriter

from nerf.data import load_dataset, compute_rays
from nerf.models import NeRFModel, SirenNeRFModel
from nerf.rendering import render_nerf
from nerf.loss import mse_to_psnr


def parse_config(config_path: str) -> dict:
    """
    Parse a configuration file where each non-empty, non-comment line is of the format:
        key = value  # optional inline comment
    Returns a dictionary mapping keys to values.
    """
    config = {}
    with open(config_path, 'r') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            # Remove inline comments
            line = line.split('#', 1)[0].strip()

            # Skip lines that become empty after removing comments
            if not line:
                continue

            if '=' in line:
                key, value = line.split('=', maxsplit=1)
                config[key.strip()] = value.strip()
            else:
                print(f"Warning: Invalid line in config file: {line}")

    return config


def format_elapsed_time(start_time: datetime.datetime) -> str:
    """
    Compute the elapsed time since start_time and format it as HH:MM:SS.
    """
    elapsed_time = datetime.datetime.now() - start_time
    total_seconds = int(elapsed_time.total_seconds())
    return '{:02d}:{:02d}:{:02d}'.format(
        total_seconds // 3600,
        (total_seconds % 3600) // 60,
        total_seconds % 60
    )


def main():
    # Load configuration
    parser = argparse.ArgumentParser(
        description="Train NeRF on a given dataset using volumetric rendering."
    )
    parser.add_argument('--config', type=str, required=True,
                        help='Path to configuration file (e.g. config_lego.txt)')
    args = parser.parse_args()
    config = parse_config(args.config)

    # Dataset parameters
    dataset_path = config.get('dataset_path', './datasets/lego')

    # Sampling Parameters
    num_random_rays = int(config.get('num_random_rays', 1024))
    chunk_size = int(config.get('chunk_size', 8192))
    num_samples = int(config.get('num_samples', 256))

    # Training Parameters
    num_iters = int(config.get('num_iters', 150000))
    learning_rate = float(config.get('learning_rate', 5e-4))
    near = float(config.get('near', 2.0))
    far = float(config.get('far', 6.0))

    # Model saving parameters
    save_path = config.get('save_path', './models')
    save_interval = int(config.get('save_interval', 5000))
    os.makedirs(save_path, exist_ok=True)

    # Learning rate decay parameters
    lr_decay = float(config.get('lr_decay', 150))
    lr_decay_factor = float(config.get('lr_decay_factor', 0.1))

    # Model type
    model_type = config.get('model_type', 'NeRF')
    
    print("===== Training Configuration Summary =====")
    print(f"Dataset path: {dataset_path}")
    print(f"Number of random rays: {num_random_rays}")
    print(f"Chunk size: {chunk_size}")
    print(f"Number of samples: {num_samples}")
    print(f"Number of iterations: {num_iters}")
    print(f"Learning rate: {learning_rate}")
    print(f"Near plane: {near}")
    print(f"Far plane: {far}")
    print(f"Save path: {save_path}")
    print(f"Save interval: {save_interval}")
    print(f"LR decay: {lr_decay}")
    print(f"LR decay factor: {lr_decay_factor}")
    print(f"Model type: {model_type}")
    print("==========================================")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    if model_type == 'nerf':
        model = NeRFModel().to(device)
    elif model_type == 'siren':
        model = SirenNeRFModel().to(device)
    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Load the full training dataset
    images_np, c2w_matrices_np, focal_length = load_dataset(dataset_path, mode='train')
    rays_o, rays_d, target_pixels = compute_rays(images_np, c2w_matrices_np, focal_length)
    N = images_np.shape[0]

    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    mse_loss = nn.MSELoss()

    # Learning rate scheduler with exponential decay
    gamma = lr_decay_factor ** (1 / (lr_decay * 1000))
    scheduler = ExponentialLR(optimizer, gamma=gamma)

    start_time = datetime.datetime.now()

    os.makedirs('./logs', exist_ok=True)
    writer = SummaryWriter(log_dir='./logs')
    writer.add_text('config', str(config))

    # Training loop with tqdm progress bar
    for step in tqdm(range(num_iters), desc="Training", unit="it"):
        # Randomly select an image from the dataset
        img_idx = np.random.randint(0, N)

        # Get the rays and target pixels for the selected image
        rays_o_image_np = rays_o[img_idx]
        rays_d_image_np = rays_d[img_idx]
        target_pixels_image_np = target_pixels[img_idx]
        rays_o_image = torch.from_numpy(rays_o_image_np).float().to(device).squeeze(0)
        rays_d_image = torch.from_numpy(rays_d_image_np).float().to(device).squeeze(0)
        target_pixels_image = torch.from_numpy(target_pixels_image_np).float().to(device).squeeze(0)

        # Randomly sample a subset of rays for this iteration
        num_pixels = rays_o_image.shape[0]
        sel_inds = np.random.choice(num_pixels, size=num_random_rays, replace=False)
        rays_o_batch = rays_o_image[sel_inds]
        rays_d_batch = rays_d_image[sel_inds]
        target_rgb = target_pixels_image[sel_inds]

        # Use render_volume to compute the predicted color along each ray
        pred_rgb = render_nerf(
            model,
            rays_o_batch,
            rays_d_batch,
            near,
            far,
            num_samples = num_samples,
            device = device,
            white_background = True,
            chunk_size = chunk_size
        )

        # Compute loss and update the model
        optimizer.zero_grad()
        loss = mse_loss(pred_rgb, target_rgb)
        loss.backward()
        optimizer.step()
        scheduler.step()

        # Log loss and PSNR
        if step % 10 == 0:
            current_lr = scheduler.get_last_lr()[0]
            elapsed_str = format_elapsed_time(start_time)
            log_message = (f"[{elapsed_str}] [Iter {step:07d}] LR: {current_lr:.6f} "
                           f"MSE: {loss.item():.4f} PSNR: {mse_to_psnr(loss.item()):.2f}")
            tqdm.write(log_message)

            # Log loss and PSNR to TensorBoard
            writer.add_scalar('loss', loss.item(), step)
            writer.add_scalar('psnr', mse_to_psnr(loss.item()), step)

        # Save model checkpoint
        if step % save_interval == 0 and step > 0:
            model_filename = os.path.join(save_path, f"{model_type}_model_{step:07d}.pth")
            torch.save(model.state_dict(), model_filename)
            elapsed_str = format_elapsed_time(start_time)
            tqdm.write(f"[{elapsed_str}] Model saved to {model_filename} at iteration {step}")

    # Save final model
    final_model_path = os.path.join(save_path, f"{model_type}_model_final.pth")
    torch.save(model.state_dict(), final_model_path)
    elapsed_str = format_elapsed_time(start_time)
    tqdm.write(f"[{elapsed_str}] Training complete!")
    tqdm.write(f"[{elapsed_str}] Final model saved to {final_model_path}")


if __name__ == '__main__':
    main()
# ./eval.py

import time
import argparse
import numpy as np
import torch
import matplotlib.pyplot as plt
from nerf.data import load_dataset, compute_rays
from nerf.models import NeRFModel
from nerf.rendering import render_nerf


def parse_config(config_path: str) -> dict:
    """
    Parse a configuration file where each non-empty, non-comment line is of the format:
        key = value  # optional inline comment
    Returns a dictionary mapping keys to values.
    """
    config = {}
    with open(config_path, 'r') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            # Remove inline comments
            line = line.split('#', 1)[0].strip()

            # Skip lines that become empty after removing comments
            if not line:
                continue

            if '=' in line:
                key, value = line.split('=', maxsplit=1)
                config[key.strip()] = value.strip()
            else:
                print(f"Warning: Invalid line in config file: {line}")

    return config


def main():
    # Load configuration
    parser = argparse.ArgumentParser(
        description="Evaluate NeRF: Render a test image using a saved checkpoint."
    )
    parser.add_argument('--config', type=str, required=True,
                        help='Path to configuration file (e.g. config_lego.txt)')
    args = parser.parse_args()
    config = parse_config(args.config)

    # Load all necessary parameters from the config file
    dataset_path = config.get('datadir', './datasets/lego')
    checkpoint_path = config.get('checkpoint_path', None)
    if checkpoint_path is None:
        raise ValueError("Please provide a checkpoint_path in the config file.")
    near = float(config.get('near', 2.0))
    far = float(config.get('far', 6.0))
    num_samples = int(config.get('num_samples', 256))
    chunk_size = int(config.get('chunk_size', 16384))
    white_background = config.get('white_background', 'True').lower() in ['true', '1', 'yes']
    test_image_index = int(config.get('test_image_index', 0))

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {torch.cuda.get_device_name(0) if device == 'cuda' else 'CPU'}")

    # Load dataset in test mode
    images_np, c2w_matrices_np, focal_length = load_dataset(dataset_path, mode='test')
    N, H, W, _ = images_np.shape
    print(f"Loaded {N} test images of resolution {H}x{W}.")

    # Select the test image using the specified index and compute rays
    single_image = images_np[test_image_index:test_image_index+1]
    single_c2w = c2w_matrices_np[test_image_index:test_image_index+1]
    rays_o_np, rays_d_np, _ = compute_rays(single_image, single_c2w, focal_length)
    rays_o = torch.from_numpy(rays_o_np).float().to(device).squeeze(0)
    rays_d = torch.from_numpy(rays_d_np).float().to(device).squeeze(0)

    # Load the saved model checkpoint
    model = NeRFModel().to(device)
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
    model.load_state_dict(checkpoint)
    model.eval()

    # Render the test image
    start_time = time.perf_counter()
    with torch.no_grad():
        pred_rgb = render_nerf(
            model,
            rays_o,
            rays_d,
            near,
            far,
            num_samples=num_samples,
            device=device,
            white_background=white_background,
            chunk_size=chunk_size
        )
        end_time = time.perf_counter()
    inference_time = end_time - start_time
    print(f"Inference took {inference_time:.2f} seconds.")

    # Reshape the predicted rays into an image
    rendered_image = pred_rgb.cpu().numpy().reshape(H, W, 3)

    # Compute PSNR between the rendered image and the ground truth test image
    gt_image = single_image[0]
    mse = np.mean((rendered_image - gt_image) ** 2)
    psnr = -10.0 * np.log10(mse)
    print("PSNR: {:.2f}".format(psnr))

    # Plot the rendered image
    plt.figure(figsize=(8, 8))
    plt.imshow(rendered_image)
    plt.axis('off')
    plt.title('Rendered Test Image')
    plt.show()


if __name__ == '__main__':
    main()
# nerf/data.py

import os
import json
import numpy as np
import imageio.v2 as imageio
from typing import Tuple


def load_dataset(dataset_path: str, mode: str = 'train') -> Tuple[np.ndarray, np.ndarray, float]:
    """
    Load images and camera-to-world transformation matrices from the dataset.

    This function reads a JSON file containing camera parameters and frame information,
    loads each corresponding image, and composites images with an alpha channel over a 
    white background. It also computes the camera's focal length based on the horizontal
    field of view.

    Args:
        dataset_path (str): Base directory of the dataset.
        mode (str): Dataset split mode ('train', 'val', or 'test'). Defaults to 'train'.

    Returns:
        Tuple[np.ndarray, np.ndarray, float]:
            - images: Array of shape (N, H, W, 3) with normalized RGB images.
            - c2w_matrices: Array of shape (N, 4, 4) with camera-to-world transformation matrices.
            - focal_length: Focal length computed from the camera's field of view.
    """
    transforms_path = os.path.join(dataset_path, f"transforms_{mode}.json")
    with open(transforms_path, 'r') as f:
        meta = json.load(f)

    camera_angle_x = meta["camera_angle_x"]
    frames = meta["frames"]

    images = []
    c2w_matrices = []
    for frame in frames:
        rel_path = frame["file_path"].lstrip("./")
        img_path = os.path.join(dataset_path, rel_path + ".png")
        img = imageio.imread(img_path).astype(np.float32) / 255.0
        
        # Composite image with alpha channel over white background if applicable
        if img.shape[-1] == 4:
            alpha = img[..., 3:4]
            img = img[..., :3] * alpha + (1.0 - alpha)
        
        images.append(img)
        c2w_matrices.append(np.array(frame["transform_matrix"], dtype=np.float32))
    
    images = np.stack(images, axis=0)
    c2w_matrices = np.stack(c2w_matrices, axis=0)
    _, _, W, _ = images.shape

    focal_length = 0.5 * W / np.tan(0.5 * camera_angle_x)
    
    return images, c2w_matrices, focal_length


def compute_rays(images: np.ndarray, c2w_matrices: np.ndarray, focal_length: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Compute camera ray origins and directions, and extract target pixel colors.

    This function generates a meshgrid of pixel coordinates, converts these coordinates 
    into camera space, and applies the camera-to-world rotation and translation to compute 
    ray origins and normalized ray directions for each pixel. It also flattens the target 
    pixel colors for subsequent processing.

    Args:
        images (np.ndarray): Array of shape (N, H, W, 3) with RGB images.
        c2w_matrices (np.ndarray): Array of shape (N, 4, 4) with camera-to-world matrices.
        focal_length (float): Focal length derived from the camera intrinsics.

    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray]:
            - rays_o: Ray origins with shape (N, H*W, 3).
            - rays_d: Normalized ray directions with shape (N, H*W, 3).
            - target_pixels: Flattened RGB pixel colors with shape (N, H*W, 3).
    """
    N, H, W, _ = images.shape

    target_pixels = images.reshape(N, -1, 3)

    u = np.arange(W, dtype=np.float32)
    v = np.arange(H, dtype=np.float32)
    u_grid, v_grid = np.meshgrid(u, v, indexing='xy')

    # Convert pixel coordinates to camera space
    x_cam = u_grid - 0.5 * W
    y_cam = -(v_grid - 0.5 * H)
    z_cam = -np.full_like(x_cam, focal_length)
    directions_cam = np.stack([x_cam, y_cam, z_cam], axis=-1)

    R = c2w_matrices[:, :3, :3]
    t = c2w_matrices[:, :3, 3]

    # Vectorized application of camera-space directions transformation
    rays_d = np.einsum('nij,hwj->nhwi', R, directions_cam)
    
    # Normalize ray directions
    rays_d = rays_d / np.linalg.norm(rays_d, axis=-1, keepdims=True)

    # Replicate camera origin for each ray
    rays_o = np.tile(t[:, None, None, :], (1, H, W, 1))

    rays_o = rays_o.reshape(N, -1, 3)
    rays_d = rays_d.reshape(N, -1, 3)

    return rays_o, rays_d, target_pixels
# nerf/encoding.py

import torch


def positional_encoding(x: torch.Tensor, L: int) -> torch.Tensor:
    """
    Applies positional encoding to the input tensor.

    Args:
        x (torch.Tensor): Input tensor.
        L (int): Number of encoding functions (frequencies).

    Returns:
        torch.Tensor: Concatenated tensor with positional encodings.
    """
    out = [x]
    for j in range(L):
        out.append(torch.sin(2 ** j * x))
        out.append(torch.cos(2 ** j * x))
    return torch.cat(out, dim=1)
# nerf/loss.py

import numpy as np


def mse_to_psnr(mse: float) -> float:
    """
    Converts Mean Squared Error to Peak Signal-to-Noise Ratio.

    Args:
        mse (float): Mean Squared Error.

    Returns:
        float: PSNR value.
    """
    return 20 * np.log10(1 / np.sqrt(mse))
# nerf/models.py

import torch
import torch.nn as nn
import numpy as np
from typing import Tuple
from nerf.encoding import positional_encoding


class NeRFModel(nn.Module):
    """
    Standard NeRF model using ReLU activations and positional encoding.
    """
    def __init__(self, pos_encoding_dim: int = 10, dir_encoding_dim: int = 4, hidden_dim: int = 256) -> None:
        super(NeRFModel, self).__init__()

        # First MLP block: processes positional encoded 3D points
        self.block1 = nn.Sequential(
            nn.Linear(pos_encoding_dim * 6 + 3, hidden_dim),  # 3D point input + 3 sine/cosine pairs per frequency
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Second MLP block: refines features and predicts density
        self.block2 = nn.Sequential(
            nn.Linear(hidden_dim + pos_encoding_dim * 6 + 3, hidden_dim),  # Skip connection with original point
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim + 1)  # Last neuron outputs density
        )

        # RGB head: predicts view-dependent color from features and encoded ray direction
        self.rgb_head = nn.Sequential(
            nn.Linear(hidden_dim + dir_encoding_dim * 6 + 3, hidden_dim // 2),  # Concatenate with ray direction
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 3),
            nn.Sigmoid()  # Map output to [0, 1]
        )

        self.pos_encoding_dim = pos_encoding_dim
        self.dir_encoding_dim = dir_encoding_dim

    def forward(self, points: torch.Tensor, rays_d: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass: encode inputs, compute features, and output RGB color and density.
        """
        points_enc = positional_encoding(points, self.pos_encoding_dim)
        rays_d_enc = positional_encoding(rays_d, self.dir_encoding_dim)

        features = self.block1(points_enc)
        features = self.block2(torch.cat((features, points_enc), dim=1))
        density = torch.relu(features[:, -1])
        features = features[:, :-1]
        colors = self.rgb_head(torch.cat((features, rays_d_enc), dim=1))
        return colors, density


class SineLayer(nn.Module):
    """
    Sine activation layer with a scaling factor.
    """
    def __init__(self, w0: float):
        super(SineLayer, self).__init__()
        self.w0 = w0

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.sin(self.w0 * x)


class SirenNeRFModel(nn.Module):
    """
    NeRF variant using SIREN activations.
    Processes raw 3D points and ray directions with sine-based MLPs.
    """
    def __init__(self, w0: float = 30, hidden_w0: float = 1, hidden_dim: int = 256):
        super(SirenNeRFModel, self).__init__()
        
        # First MLP block: processes raw 3D point coordinates
        self.block1 = nn.Sequential(
            nn.Linear(3, hidden_dim),  # 3D point input
            SineLayer(w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0)
        )

        # Second MLP block: refines features and predicts density
        self.block2 = nn.Sequential(
            nn.Linear(hidden_dim + 3, hidden_dim),  # Skip connection with original point
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim + 1)  # Last neuron outputs density
        )

        # RGB head: predicts view-dependent color from features and ray direction
        self.rgb_head = nn.Sequential(
            nn.Linear(hidden_dim + 3, hidden_dim // 2),  # Concatenate with ray direction
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim // 2, 3),
            nn.Sigmoid()  # Map output to [0, 1]
        )

        # Weight initialization for SIREN layers
        # First layer: U(-1/in_features, 1/in_features)
        # Subsequent layers: U(-sqrt(6/in_features)/hidden_w0, sqrt(6/in_features)/hidden_w0)
        with torch.no_grad():
            # Initialize block1 linear layers
            for i, module in enumerate(self.block1):
                if isinstance(module, nn.Linear):
                    if i == 0:  # first layer of block1
                        bound = 1 / module.in_features
                    else:
                        bound = np.sqrt(6 / module.in_features) / hidden_w0
                    module.weight.uniform_(-bound, bound)
            
            # Initialize block2 linear layers
            for i, module in enumerate(self.block2):
                if isinstance(module, nn.Linear):
                    bound = np.sqrt(6 / module.in_features) / hidden_w0
                    module.weight.uniform_(-bound, bound)
            
            # Initialize RGB head first linear layer
            for i, module in enumerate(self.rgb_head):
                if isinstance(module, nn.Linear):
                    if i == 0:
                        bound = np.sqrt(6 / module.in_features) / hidden_w0
                        module.weight.uniform_(-bound, bound)

    def forward(self, points: torch.Tensor, rays_d: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass: encode 3D points, compute features, and output RGB color and density.
        """
        features = self.block1(points)
        features = self.block2(torch.cat((features, points), dim=1))
        density = torch.relu(features[:, -1])
        features = features[:, :-1]
        colors = self.rgb_head(torch.cat((features, rays_d), dim=1))
        return colors, density
# nerf/rendering.py

import torch
from torch import Tensor
from typing import Tuple


def stratified_sampling(
    near: float,
    far: float,
    num_bins: int,
    device: str = 'cpu'
) -> Tensor:
    """
    Perform stratified sampling within a given depth range.

    Args:
        near (float): Near bound of the sampling range.
        far (float): Far bound of the sampling range.
        num_bins (int): Number of samples.
        device (str): Device for computation.

    Returns:
        Tensor: Stratified samples within [near, far].
    """
    bins = torch.linspace(near, far, num_bins + 1, device=device)
    lower = bins[:-1]
    upper = bins[1:]
    random_offsets = torch.rand(num_bins, device=device)
    return lower + (upper - lower) * random_offsets


def generate_sample_positions(
    rays_o_batch: Tensor,
    rays_d_batch: Tensor,
    near: float,
    far: float,
    num_samples: int,
    device: str = 'cpu'
) -> Tuple[Tensor, Tensor]:
    """
    Generate stratified sample positions for a batch of rays and compute sample intervals.

    Args:
        rays_o_batch (Tensor): Ray origins.
        rays_d_batch (Tensor): Ray directions.
        near (float): Near bound for sampling.
        far (float): Far bound for sampling.
        num_samples (int): Number of samples per ray.
        device (str): Computation device.

    Returns:
        Tuple[Tensor, Tensor]:
            - sample_positions (Tensor): Sample positions for each ray.
            - deltas (Tensor): Intervals between sampled positions.
    """
    strat_samples = stratified_sampling(near, far, num_samples, device)
    deltas = strat_samples[1:] - strat_samples[:-1]

    # Append a large interval for the last sample
    delta_inf = torch.tensor([1e10], device=deltas.device, dtype=deltas.dtype)
    deltas = torch.cat([deltas, delta_inf], dim=0)

    # Expand each ray to get sample positions
    sample_positions = (
        rays_o_batch.unsqueeze(1)
        + strat_samples.unsqueeze(0).unsqueeze(-1) * rays_d_batch.unsqueeze(1)
    )

    return sample_positions, deltas


def normalize_positions(
    positions: Tensor,
    near: float,
    far: float
) -> Tensor:
    """
    Normalize positions to the range [-1, 1].

    Args:
        positions (Tensor): Sampled positions.
        near (float): Near bound.
        far (float): Far bound.

    Returns:
        Tensor: Normalized positions.
    """
    return 2 * (positions - near) / (far - near) - 1


def query_model(
    model: torch.nn.Module,
    sample_positions_flat: Tensor,
    directions_flat: Tensor,
    near: float,
    far: float
) -> Tuple[Tensor, Tensor]:
    """
    Normalize sample positions and query the model to obtain colors and densities.

    Args:
        model (torch.nn.Module): NeRF model.
        sample_positions_flat (Tensor): Flattened sample positions.
        directions_flat (Tensor): Flattened ray directions.
        near (float): Near bound for normalization.
        far (float): Far bound for normalization.

    Returns:
        Tuple[Tensor, Tensor]: 
            - colors (Tensor): Predicted colors.
            - densities (Tensor): Predicted densities.
    """
    sample_positions_normalized = normalize_positions(sample_positions_flat, near, far)
    return model.forward(sample_positions_normalized, directions_flat)


def compute_accumulated_transmittance(betas: Tensor) -> Tensor:
    """
    Compute cumulative transmittance along rays.

    Args:
        betas (Tensor): Complement of alpha values (1 - alpha) for each sample.

    Returns:
        Tensor: Accumulated transmittance along each ray.
    """
    accum_trans = torch.cumprod(betas, dim=1)
    # Prepend a 1 so the transmittance for the first sample is 1
    init = torch.ones(accum_trans.shape[0], 1, device=accum_trans.device)
    return torch.cat((init, accum_trans[:, :-1]), dim=1)


def composite_volume(
    colors: Tensor,
    densities: Tensor,
    deltas: Tensor,
    white_background: bool
) -> Tensor:
    """
    Composite colors along each ray using volumetric rendering.

    Args:
        colors (Tensor): Colors predicted by the model.
        densities (Tensor): Densities predicted by the model.
        deltas (Tensor): Intervals between sampled positions.
        white_background (bool): Flag indicating whether to composite with a white background.

    Returns:
        Tensor: Composite RGB colors for each ray.
    """
    # alpha_i = 1 - exp(-sigma_i * delta_i)
    alpha = 1 - torch.exp(-densities * deltas.unsqueeze(0))

    # weights_i = T_i * alpha_i
    weights = compute_accumulated_transmittance(1 - alpha) * alpha

    # Final composite color
    comp_rgb = (weights.unsqueeze(-1) * colors).sum(dim=1)

    # If we assume a white background, add the remainder
    if white_background:
        comp_rgb = comp_rgb + (1 - weights.sum(dim=1, keepdim=True))

    return comp_rgb


def render_nerf(
    model: torch.nn.Module,
    rays_o: Tensor,
    rays_d: Tensor,
    near: float,
    far: float,
    num_samples: int = 256,
    device: str = 'cpu',
    white_background: bool = True,
    chunk_size: int = 8192
) -> Tensor:
    """
    Render rays via volumetric rendering using a NeRF model.

    Args:
        model (torch.nn.Module): NeRF model.
        rays_o (Tensor): Ray origins.
        rays_d (Tensor): Ray directions.
        near (float): Near bound.
        far (float): Far bound.
        num_samples (int, optional): Number of samples per ray.
        device (str, optional): Computation device.
        white_background (bool, optional): Use a white background.
        chunk_size (int, optional): Number of rays to process per chunk.

    Returns:
        Tensor: Rendered RGB colors for each ray.
    """
    # Ensure data is on the specified device
    rays_o = rays_o.to(device)
    rays_d = rays_d.to(device)

    # Storage for all chunked results
    rgb_out = []

    for i in range(0, rays_o.shape[0], chunk_size):
        # Chunk the rays to avoid OOM for large batches
        rays_o_chunk = rays_o[i:i + chunk_size]
        rays_d_chunk = rays_d[i:i + chunk_size]

        # Sample positions along each ray
        sample_positions, deltas = generate_sample_positions(
            rays_o_chunk,
            rays_d_chunk,
            near,
            far,
            num_samples,
            device
        )

        # Flatten for batching into the network
        sample_positions_flat = sample_positions.reshape(-1, 3)
        directions_flat = (
            rays_d_chunk
            .unsqueeze(1)
            .expand(-1, num_samples, -1)
            .reshape(-1, 3)
        )

        # Query the model for colors & densities
        colors_flat, densities_flat = query_model(
            model,
            sample_positions_flat,
            directions_flat,
            near,
            far
        )
        
        # Reshape back to [batch, num_samples, ...]
        colors = colors_flat.reshape(rays_o_chunk.shape[0], num_samples, 3)
        densities = densities_flat.reshape(rays_o_chunk.shape[0], num_samples)

        # Composite final color per ray
        composed_rgb = composite_volume(colors, densities, deltas, white_background)
        rgb_out.append(composed_rgb)

    # Concatenate results for all chunks
    return torch.cat(rgb_out, dim=0)
