# ./train.py

import os
import argparse
import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ExponentialLR
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from nerf.data import load_dataset, compute_rays, RayDataset
from nerf.models import NeRFModel, SirenNeRFModel
from nerf.rendering import render_nerf
from nerf.loss import mse_to_psnr


def parse_config(config_path: str) -> dict:
    """
    Parse a configuration file where each non-empty, non-comment line is of the format:
        key = value  # optional inline comment
    Returns a dictionary mapping keys to values.
    """
    config = {}
    with open(config_path, 'r') as f:
        for line in f:
            line = line.strip()

            if not line or line.startswith('#'):
                continue

            line = line.split('#', 1)[0].strip()

            if not line:
                continue

            if '=' in line:
                key, value = line.split('=', maxsplit=1)
                config[key.strip()] = value.strip()
            else:
                print(f"Warning: Invalid line in config file: {line}")

    return config


def format_elapsed_time(start_time: datetime.datetime) -> str:
    """
    Compute the elapsed time since start_time and format it as HH:MM:SS.
    """
    elapsed_time = datetime.datetime.now() - start_time
    total_seconds = int(elapsed_time.total_seconds())
    return '{:02d}:{:02d}:{:02d}'.format(
        total_seconds // 3600,
        (total_seconds % 3600) // 60,
        total_seconds % 60
    )


def save_checkpoint(step, model, optimizer, scheduler, save_path, model_type, prefix=""):
    """
    Save the training checkpoint.
    """
    checkpoint_dict = {
        'step': step,
        'model_type': model_type,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict()
    }
    model_filename = os.path.join(save_path, f"{model_type}_model_{prefix}{step:07d}.pth")
    torch.save(checkpoint_dict, model_filename)
    return model_filename


def log_training_metrics(step, scheduler, loss, start_time, writer):
    """
    Log training metrics.
    """
    current_lr = scheduler.get_last_lr()[0]
    elapsed_str = format_elapsed_time(start_time)
    log_message = (f"[{elapsed_str}] [Iter {step:07d}] LR: {current_lr:.6f} "
                   f"MSE: {loss.item():.4f} PSNR: {mse_to_psnr(loss.item()):.2f}")
    tqdm.write(log_message)
    writer.add_scalar('loss', loss.item(), step)
    writer.add_scalar('psnr', mse_to_psnr(loss.item()), step)


def main():
    # Set random seed for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)

    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description="Train NeRF on a given dataset using volumetric rendering."
    )
    parser.add_argument('--config', type=str, required=True,
                        help='Path to configuration file')
    parser.add_argument('--resume', type=str, default=None,
                        help='Path to a checkpoint file to resume training from')
    args = parser.parse_args()
    config = parse_config(args.config)

    # Dataset parameters
    dataset_path = config.get('dataset_path', './datasets/lego')

    # Sampling Parameters
    num_random_rays = int(config.get('num_random_rays', 1024))
    chunk_size = int(config.get('chunk_size', 8192))
    num_samples = int(config.get('num_samples', 256))

    # Training Parameters
    num_iters = int(config.get('num_iters', 150000))
    learning_rate = float(config.get('learning_rate', 5e-4))
    near = float(config.get('near', 2.0))
    far = float(config.get('far', 6.0))

    # Model saving parameters
    save_path = config.get('save_path', './models')
    save_interval = int(config.get('save_interval', 5000))
    os.makedirs(save_path, exist_ok=True)

    # Learning rate decay parameters
    lr_decay = float(config.get('lr_decay', 150))
    lr_decay_factor = float(config.get('lr_decay_factor', 0.1))

    # Model type
    if args.resume is not None:
        checkpoint_temp = torch.load(args.resume, map_location='cpu')
        model_type = checkpoint_temp.get('model_type', config.get('model_type', 'NeRF')).lower()
        print(f"Resuming training with model type from checkpoint: {model_type}")
    else:
        model_type = config.get('model_type', 'NeRF').lower()

    # Monitoring parameters
    log_interval = int(config.get('log_interval', 10))
    val_interval = int(config.get('val_interval', 1000))
    
    print("===== Training Configuration Summary =====")
    print(f"Dataset path: {dataset_path}")
    print(f"Number of random rays: {num_random_rays}")
    print(f"Chunk size: {chunk_size}")
    print(f"Number of samples: {num_samples}")
    print(f"Number of iterations: {num_iters}")
    print(f"Learning rate: {learning_rate}")
    print(f"Near plane: {near}")
    print(f"Far plane: {far}")
    print(f"Save path: {save_path}")
    print(f"Save interval: {save_interval}")
    print(f"LR decay: {lr_decay}")
    print(f"LR decay factor: {lr_decay_factor}")
    print(f"Log interval: {log_interval}")
    print(f"Validation interval: {val_interval}")
    print(f"Model type: {model_type}")
    print("==========================================")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {torch.cuda.get_device_name(0) if device == 'cuda' else 'CPU'}")
    if model_type == 'nerf':
        model = NeRFModel().to(device)
    elif model_type == 'siren':
        model = SirenNeRFModel().to(device)
    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Load the training dataset
    print("Loading training dataset...")
    images_np, c2w_matrices_np, focal_length = load_dataset(dataset_path, mode='train')
    rays_o, rays_d, target_pixels = compute_rays(images_np, c2w_matrices_np, focal_length)

    # Load the validation dataset
    print("Loading validation dataset...")
    images_val_np, c2w_val_np, focal_length_val = load_dataset(dataset_path, mode='test')
    N_val, H_val, W_val, _ = images_np.shape
    print(f"Loaded {N_val} validation images of resolution {H_val}x{W_val}.")

    # Create the dataset and DataLoader
    dataset = RayDataset(rays_o, rays_d, target_pixels)
    data_loader = DataLoader(dataset, batch_size=num_random_rays, shuffle=True)
    loader_iter = iter(data_loader)

    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    mse_loss = nn.MSELoss()

    # Learning rate scheduler
    gamma = lr_decay_factor ** (1 / (lr_decay * 1000))
    scheduler = ExponentialLR(optimizer, gamma=gamma)

    # TensorBoard writer
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    log_dir = f"./logs/{model_type}_{os.path.basename(dataset_path)}_{timestamp}"
    os.makedirs(log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir=log_dir)
    writer.add_text('config', str(config))

    # Resume training from checkpoint
    start_iter = 0
    start_time = datetime.datetime.now()
    if args.resume is not None:
        checkpoint = torch.load(args.resume, map_location='cpu', weights_only=True)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_iter = checkpoint['step']
        print(f"Resuming training from iteration {start_iter}")

    # Training loop
    try:
        with tqdm(total=num_iters, initial=start_iter, desc="Training", unit="it") as pbar:
            for step in range(start_iter, num_iters):
                try:
                    rays_o_batch, rays_d_batch, target_rgb_batch = next(loader_iter)
                except StopIteration:
                    # Reinitialize iterator if the DataLoader is exhausted
                    loader_iter = iter(data_loader)
                    rays_o_batch, rays_d_batch, target_rgb_batch = next(loader_iter)
                
                rays_o_batch = rays_o_batch.to(device)
                rays_d_batch = rays_d_batch.to(device)
                target_rgb_batch = target_rgb_batch.to(device)
                
                pred_rgb = render_nerf(
                    model,
                    rays_o_batch,
                    rays_d_batch,
                    near,
                    far,
                    num_samples=num_samples,
                    device=device,
                    white_background=True,
                    chunk_size=chunk_size
                )

                # Compute loss, backpropagate, and update model
                optimizer.zero_grad()
                loss = mse_loss(pred_rgb, target_rgb_batch)
                loss.backward()
                optimizer.step()
                scheduler.step()

                # Log metrics and write to TensorBoard
                if step % log_interval == 0:
                    log_training_metrics(step, scheduler, loss, start_time, writer)

                # Save checkpoint
                if step % save_interval == 0 and step > 0 and step < num_iters - 1:
                    model_filename = save_checkpoint(step, model, optimizer, scheduler, save_path, model_type)
                    elapsed_str = format_elapsed_time(start_time)
                    tqdm.write(f"[{elapsed_str}] Model saved to {model_filename} at iteration {step}")

                # Log validation metrics
                if step % val_interval == 0:
                    # Select a random image and render it
                    test_image_index = np.random.randint(N_val)
                    single_val_image = images_val_np[test_image_index:test_image_index+1]
                    single_val_c2w = c2w_val_np[test_image_index:test_image_index+1]
                    rays_o_val_np, rays_d_val_np, _ = compute_rays(single_val_image, single_val_c2w, focal_length_val)
                    rays_o_val = torch.from_numpy(rays_o_val_np).float().to(device).squeeze(0)
                    rays_d_val = torch.from_numpy(rays_d_val_np).float().to(device).squeeze(0)

                    tqdm.write("Rendering validation image...")
                    
                    model.eval()
                    torch.cuda.empty_cache()
                    with torch.no_grad():
                        pred_val_rgb = render_nerf(
                            model,
                            rays_o_val,
                            rays_d_val,
                            near,
                            far,
                            num_samples=num_samples,
                            device=device,
                            white_background=True,
                            chunk_size=chunk_size
                        )
                    model.train()
                    
                    # Reshape to image
                    H_v, W_v = single_val_image.shape[1:3]
                    pred_val_rgb = pred_val_rgb.reshape(H_v, W_v, 3).cpu().numpy()
                    tqdm.write(f"Validation Debug: Rendered image shape: {pred_val_rgb.shape}")
                    
                    # Compute validation PSNR
                    gt_val_img = single_val_image[0]
                    val_mse = np.mean((pred_val_rgb - gt_val_img) ** 2)
                    val_psnr = mse_to_psnr(val_mse)
                    tqdm.write(f"Validation Debug: MSE = {val_mse:.4f}, PSNR = {val_psnr:.2f}")
                    writer.add_scalar("val/psnr", val_psnr, step)
                    
                    # Log the rendered image as a TensorBoard image
                    pred_val_rgb_clamped = np.clip(pred_val_rgb, 0.0, 1.0)
                    writer.add_image(
                        "val/render",
                        torch.from_numpy(pred_val_rgb_clamped).permute(2, 0, 1),
                        step
                    )
                    
                    tqdm.write(f"Validation Debug: Logging complete for iteration {step}.")
                    tqdm.write(f"[Validation Step] Iter {step}  PSNR: {val_psnr:.2f}")

                pbar.update(1)

            # Save final model after training is complete
            final_model_path = save_checkpoint(num_iters, model, optimizer, scheduler, save_path, model_type, prefix="final_")
            elapsed_str = format_elapsed_time(start_time)
            tqdm.write(f"[{elapsed_str}] Training complete!")
            tqdm.write(f"[{elapsed_str}] Final model saved to {final_model_path}")

    except KeyboardInterrupt:
        # Save checkpoint on keyboard interrupt
        elapsed_str = format_elapsed_time(start_time)
        tqdm.write(f"\n[{elapsed_str}] Keyboard interrupt detected! Saving current checkpoint...")
        interrupt_checkpoint_path = save_checkpoint(step, model, optimizer, scheduler, save_path, model_type, prefix="interrupt_")
        tqdm.write(f"[{elapsed_str}] Checkpoint saved to {interrupt_checkpoint_path}. Exiting training.")


if __name__ == '__main__':
    main()
# nerf/data.py

import os
import json
import numpy as np
import imageio.v2 as imageio
import torch
from torch.utils.data import Dataset
from typing import Tuple


def load_dataset(dataset_path: str, mode: str = 'train') -> Tuple[np.ndarray, np.ndarray, float]:
    """
    Load images and camera-to-world transformation matrices from the dataset.

    This function reads a JSON file containing camera parameters and frame information,
    loads each corresponding image, and composites images with an alpha channel over a 
    white background. It also computes the camera's focal length based on the horizontal
    field of view.

    Args:
        dataset_path (str): Base directory of the dataset.
        mode (str): Dataset split mode ('train', 'val', or 'test'). Defaults to 'train'.

    Returns:
        Tuple[np.ndarray, np.ndarray, float]:
            - images: Array of shape (N, H, W, 3) with normalized RGB images.
            - c2w_matrices: Array of shape (N, 4, 4) with camera-to-world transformation matrices.
            - focal_length: Focal length computed from the camera's field of view.
    """
    transforms_path = os.path.join(dataset_path, f"transforms_{mode}.json")
    with open(transforms_path, 'r') as f:
        meta = json.load(f)

    camera_angle_x = meta["camera_angle_x"]
    frames = meta["frames"]

    images = []
    c2w_matrices = []
    for frame in frames:
        rel_path = frame["file_path"].lstrip("./")
        img_path = os.path.join(dataset_path, rel_path + ".png")
        img = imageio.imread(img_path).astype(np.float32) / 255.0
        
        # Composite image with alpha channel over white background if applicable
        if img.shape[-1] == 4:
            alpha = img[..., 3:4]
            img = img[..., :3] * alpha + (1.0 - alpha)
        
        images.append(img)
        c2w_matrices.append(np.array(frame["transform_matrix"], dtype=np.float32))
    
    images = np.stack(images, axis=0)
    c2w_matrices = np.stack(c2w_matrices, axis=0)
    _, _, W, _ = images.shape

    focal_length = 0.5 * W / np.tan(0.5 * camera_angle_x)
    
    return images, c2w_matrices, focal_length


def compute_rays(images: np.ndarray, c2w_matrices: np.ndarray, focal_length: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Compute camera ray origins and directions, and extract target pixel colors.

    This function generates a meshgrid of pixel coordinates, converts these coordinates 
    into camera space, and applies the camera-to-world rotation and translation to compute 
    ray origins and normalized ray directions for each pixel. It also flattens the target 
    pixel colors for subsequent processing.

    Args:
        images (np.ndarray): Array of shape (N, H, W, 3) with RGB images.
        c2w_matrices (np.ndarray): Array of shape (N, 4, 4) with camera-to-world matrices.
        focal_length (float): Focal length derived from the camera intrinsics.

    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray]:
            - rays_o: Ray origins with shape (N, H*W, 3).
            - rays_d: Normalized ray directions with shape (N, H*W, 3).
            - target_pixels: Flattened RGB pixel colors with shape (N, H*W, 3).
    """
    N, H, W, _ = images.shape

    target_pixels = images.reshape(N, -1, 3)

    u = np.arange(W, dtype=np.float32)
    v = np.arange(H, dtype=np.float32)
    u_grid, v_grid = np.meshgrid(u, v, indexing='xy')

    # Convert pixel coordinates to camera space
    x_cam = u_grid - 0.5 * W
    y_cam = -(v_grid - 0.5 * H)
    z_cam = -np.full_like(x_cam, focal_length)
    directions_cam = np.stack([x_cam, y_cam, z_cam], axis=-1)

    R = c2w_matrices[:, :3, :3]
    t = c2w_matrices[:, :3, 3]

    # Vectorized application of camera-space directions transformation
    rays_d = np.einsum('nij,hwj->nhwi', R, directions_cam)
    
    # Normalize ray directions
    rays_d = rays_d / np.linalg.norm(rays_d, axis=-1, keepdims=True)

    # Replicate camera origin for each ray
    rays_o = np.tile(t[:, None, None, :], (1, H, W, 1))

    rays_o = rays_o.reshape(N, -1, 3)
    rays_d = rays_d.reshape(N, -1, 3)

    return rays_o, rays_d, target_pixels


class RayDataset(Dataset):
    """
    A PyTorch dataset for storing and accessing ray data.

    This dataset class converts ray origins, ray directions, and target pixel colors from 
    NumPy arrays to torch tensors. It flattens the input arrays to produce a dataset 
    where each sample corresponds to a single ray along with its associated color.

    Args:
        rays_o (np.ndarray): Array of ray origins with shape (N, H*W, 3).
        rays_d (np.ndarray): Array of ray directions with shape (N, H*W, 3).
        target_pixels (np.ndarray): Array of target RGB pixel colors with shape (N, H*W, 3).
    """
    def __init__(self, rays_o, rays_d, target_pixels):
        self.rays_o = torch.from_numpy(rays_o.reshape(-1, 3)).float()
        self.rays_d = torch.from_numpy(rays_d.reshape(-1, 3)).float()
        self.target_pixels = torch.from_numpy(target_pixels.reshape(-1, 3)).float()

    def __len__(self):
        return self.rays_o.shape[0]

    def __getitem__(self, idx):
        return self.rays_o[idx], self.rays_d[idx], self.target_pixels[idx]
# nerf/encoding.py

import torch


def positional_encoding(x: torch.Tensor, L: int) -> torch.Tensor:
    """
    Applies positional encoding to the input tensor.

    Args:
        x (torch.Tensor): Input tensor.
        L (int): Number of encoding functions (frequencies).

    Returns:
        torch.Tensor: Concatenated tensor with positional encodings.
    """
    out = [x]
    for j in range(L):
        out.append(torch.sin(2 ** j * x))
        out.append(torch.cos(2 ** j * x))
    return torch.cat(out, dim=1)
# nerf/loss.py

import numpy as np


def mse_to_psnr(mse: float) -> float:
    """
    Converts Mean Squared Error to Peak Signal-to-Noise Ratio.

    Args:
        mse (float): Mean Squared Error.

    Returns:
        float: PSNR value.
    """
    return 20 * np.log10(1 / np.sqrt(mse))
# nerf/models.py

import torch
import torch.nn as nn
import numpy as np
from typing import Tuple
from nerf.encoding import positional_encoding


class NeRFModel(nn.Module):
    """
    Standard NeRF model using ReLU activations and positional encoding.
    
    Args:
        pos_encoding_dim (int): Number of frequencies for positional encoding of 3D points.
        dir_encoding_dim (int): Number of frequencies for positional encoding of ray directions.
        hidden_dim (int): Number of neurons in the hidden layers.
    """
    def __init__(self, pos_encoding_dim: int = 10, dir_encoding_dim: int = 4, hidden_dim: int = 256) -> None:
        super(NeRFModel, self).__init__()

        # First MLP block
        self.block1 = nn.Sequential(
            nn.Linear(pos_encoding_dim * 6 + 3, hidden_dim),  # 3D point + 3 sin/cos pairs per freq
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Second MLP block
        self.block2 = nn.Sequential(
            nn.Linear(hidden_dim + pos_encoding_dim * 6 + 3, hidden_dim),  # Skip conn with original point
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim + 1)  # Last neuron outputs density
        )

        # RGB head
        self.rgb_head = nn.Sequential(
            nn.Linear(hidden_dim + dir_encoding_dim * 6 + 3, hidden_dim // 2),  # Concat with ray direction
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 3),
            nn.Sigmoid()
        )

        self.pos_encoding_dim = pos_encoding_dim
        self.dir_encoding_dim = dir_encoding_dim

    def forward(self, points: torch.Tensor, rays_d: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass: encode inputs, compute features, and output RGB color and density.

        Args:
            points (torch.Tensor): Tensor representing input 3D points.
            rays_d (torch.Tensor): Tensor representing input ray directions.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]:
                - colors: Tensor with normalized RGB values.
                - density: Tensor representing the density values.
        """
        points_enc = positional_encoding(points, self.pos_encoding_dim)
        rays_d_enc = positional_encoding(rays_d, self.dir_encoding_dim)

        features = self.block1(points_enc)
        features = self.block2(torch.cat((features, points_enc), dim=1))
        density = torch.relu(features[:, -1])
        features = features[:, :-1]
        colors = self.rgb_head(torch.cat((features, rays_d_enc), dim=1))
        return colors, density


class SineLayer(nn.Module):
    """
    Sine activation layer with a scaling factor.

    Args:
        w0 (float): Scaling factor applied to the input before the sine activation.
    """
    def __init__(self, w0: float):
        super(SineLayer, self).__init__()
        self.w0 = w0

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.sin(self.w0 * x)


class SirenNeRFModel(nn.Module):
    """
    NeRF variant using SIREN activations.
    Processes raw 3D points and ray directions with sine-based MLPs.

    Args:
        w0 (float): Initial scaling factor for the first SIREN activation.
        hidden_w0 (float): Scaling factor for subsequent SIREN activations.
        hidden_dim (int): Number of neurons in the hidden layers.
    """
    def __init__(self, w0: float = 30, hidden_w0: float = 1, hidden_dim: int = 256):
        super(SirenNeRFModel, self).__init__()
        
        # First MLP block
        self.block1 = nn.Sequential(
            nn.Linear(3, hidden_dim),  # 3D point input
            SineLayer(w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0)
        )

        # Second MLP block
        self.block2 = nn.Sequential(
            nn.Linear(hidden_dim + 3, hidden_dim),  # Skip conn with original point
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim),
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim, hidden_dim + 1)  # Last neuron outputs density
        )

        # RGB head
        self.rgb_head = nn.Sequential(
            nn.Linear(hidden_dim + 3, hidden_dim // 2),  # Concat with ray direction
            SineLayer(hidden_w0),
            nn.Linear(hidden_dim // 2, 3),
            nn.Sigmoid()
        )

        # Weight initialization for SIREN layers
        with torch.no_grad():
            # Initialize block1 linear layers
            for i, module in enumerate(self.block1):
                if isinstance(module, nn.Linear):
                    if i == 0:
                        bound = 1 / module.in_features
                    else:
                        bound = np.sqrt(6 / module.in_features) / hidden_w0
                    module.weight.uniform_(-bound, bound)
            
            # Initialize block2 linear layers
            for i, module in enumerate(self.block2):
                if isinstance(module, nn.Linear):
                    bound = np.sqrt(6 / module.in_features) / hidden_w0
                    module.weight.uniform_(-bound, bound)
            
            # Initialize RGB head first linear layer
            for i, module in enumerate(self.rgb_head):
                if isinstance(module, nn.Linear):
                    if i == 0:
                        bound = np.sqrt(6 / module.in_features) / hidden_w0
                        module.weight.uniform_(-bound, bound)

    def forward(self, points: torch.Tensor, rays_d: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass: encode 3D points, compute features, and output RGB color and density.

        Args:
            points (torch.Tensor): Tensor representing raw 3D points.
            rays_d (torch.Tensor): Tensor representing raw ray directions.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]:
                - colors: Tensor with normalized RGB values.
                - density: Tensor representing the density values.
        """
        features = self.block1(points)
        features = self.block2(torch.cat((features, points), dim=1))
        density = torch.relu(features[:, -1])
        features = features[:, :-1]
        colors = self.rgb_head(torch.cat((features, rays_d), dim=1))
        return colors, density
# nerf/rendering.py

import torch
from torch import Tensor
from tqdm import tqdm
from typing import Tuple


def stratified_sampling(
    near: float,
    far: float,
    num_bins: int,
    device: str = 'cpu'
) -> Tensor:
    """
    Perform stratified sampling within a given depth range.

    Args:
        near (float): Near bound of the sampling range.
        far (float): Far bound of the sampling range.
        num_bins (int): Number of samples.
        device (str): Device for computation.

    Returns:
        Tensor: Stratified samples within [near, far].
    """
    bins = torch.linspace(near, far, num_bins + 1, device=device)
    lower = bins[:-1]
    upper = bins[1:]
    random_offsets = torch.rand(num_bins, device=device)
    return lower + (upper - lower) * random_offsets


def generate_sample_positions(
    rays_o_batch: Tensor,
    rays_d_batch: Tensor,
    near: float,
    far: float,
    num_samples: int,
    device: str = 'cpu'
) -> Tuple[Tensor, Tensor]:
    """
    Generate stratified sample positions for a batch of rays and compute sample intervals.

    Args:
        rays_o_batch (Tensor): Ray origins.
        rays_d_batch (Tensor): Ray directions.
        near (float): Near bound for sampling.
        far (float): Far bound for sampling.
        num_samples (int): Number of samples per ray.
        device (str): Computation device.

    Returns:
        Tuple[Tensor, Tensor]:
            - sample_positions (Tensor): Sample positions for each ray.
            - deltas (Tensor): Intervals between sampled positions.
    """
    strat_samples = stratified_sampling(near, far, num_samples, device)
    deltas = strat_samples[1:] - strat_samples[:-1]

    # Append a large interval for the last sample
    delta_inf = torch.tensor([1e10], device=deltas.device, dtype=deltas.dtype)
    deltas = torch.cat([deltas, delta_inf], dim=0)

    # Expand each ray to get sample positions
    sample_positions = (
        rays_o_batch.unsqueeze(1)
        + strat_samples.unsqueeze(0).unsqueeze(-1) * rays_d_batch.unsqueeze(1)
    )

    return sample_positions, deltas


def normalize_positions(
    positions: Tensor,
    near: float,
    far: float
) -> Tensor:
    """
    Normalize positions to the range [-1, 1].

    Args:
        positions (Tensor): Sampled positions.
        near (float): Near bound.
        far (float): Far bound.

    Returns:
        Tensor: Normalized positions.
    """
    return 2 * (positions - near) / (far - near) - 1


def query_model(
    model: torch.nn.Module,
    sample_positions_flat: Tensor,
    directions_flat: Tensor,
    near: float,
    far: float
) -> Tuple[Tensor, Tensor]:
    """
    Normalize sample positions and query the model to obtain colors and densities.

    Args:
        model (torch.nn.Module): NeRF model.
        sample_positions_flat (Tensor): Flattened sample positions.
        directions_flat (Tensor): Flattened ray directions.
        near (float): Near bound for normalization.
        far (float): Far bound for normalization.

    Returns:
        Tuple[Tensor, Tensor]: 
            - colors (Tensor): Predicted colors.
            - densities (Tensor): Predicted densities.
    """
    sample_positions_normalized = normalize_positions(sample_positions_flat, near, far)
    return model.forward(sample_positions_normalized, directions_flat)


def compute_accumulated_transmittance(betas: Tensor) -> Tensor:
    """
    Compute cumulative transmittance along rays.

    Args:
        betas (Tensor): Complement of alpha values (1 - alpha) for each sample.

    Returns:
        Tensor: Accumulated transmittance along each ray.
    """
    accum_trans = torch.cumprod(betas, dim=1)
    # Prepend a 1 so the transmittance for the first sample is 1
    init = torch.ones(accum_trans.shape[0], 1, device=accum_trans.device)
    return torch.cat((init, accum_trans[:, :-1]), dim=1)


def composite_volume(
    colors: Tensor,
    densities: Tensor,
    deltas: Tensor,
    white_background: bool
) -> Tensor:
    """
    Composite colors along each ray using volumetric rendering.

    Args:
        colors (Tensor): Colors predicted by the model.
        densities (Tensor): Densities predicted by the model.
        deltas (Tensor): Intervals between sampled positions.
        white_background (bool): Flag indicating whether to composite with a white background.

    Returns:
        Tensor: Composite RGB colors for each ray.
    """
    # alpha_i = 1 - exp(-sigma_i * delta_i)
    alpha = 1 - torch.exp(-densities * deltas.unsqueeze(0))

    # weights_i = T_i * alpha_i
    weights = compute_accumulated_transmittance(1 - alpha) * alpha

    comp_rgb = (weights.unsqueeze(-1) * colors).sum(dim=1)

    if white_background:
        comp_rgb = comp_rgb + (1 - weights.sum(dim=1, keepdim=True))

    return comp_rgb


def render_nerf(
    model: torch.nn.Module,
    rays_o: Tensor,
    rays_d: Tensor,
    near: float,
    far: float,
    num_samples: int = 256,
    device: str = 'cpu',
    white_background: bool = True,
    chunk_size: int = 8192,
) -> Tensor:
    """
    Render rays with a NeRF model via volumetric integration.

    This function samples points along each ray between the specified near and far
    bounds, queries the NeRF model to obtain color and density predictions, and
    composites these predictions into a final RGB color per ray using volumetric rendering.

    Parameters:
        model (torch.nn.Module): The NeRF model that outputs colors and densities.
        rays_o (Tensor): Ray origins of shape [num_rays, 3].
        rays_d (Tensor): Ray directions of shape [num_rays, 3].
        near (float): Near bound for sampling along the rays.
        far (float): Far bound for sampling along the rays.
        num_samples (int, optional): Number of sample points per ray.
        device (str, optional): Device on which to perform rendering.
        white_background (bool, optional): If True, composite over a white background.
        chunk_size (int, optional): Number of rays processed per chunk for memory efficiency.

    Returns:
        Tensor: A tensor of shape [num_rays, 3] containing the rendered RGB colors.
    """
    rays_o = rays_o.to(device)
    rays_d = rays_d.to(device)

    rgb_out = []
    for i in range(0, rays_o.shape[0], chunk_size):
        rays_o_chunk = rays_o[i:i + chunk_size]
        rays_d_chunk = rays_d[i:i + chunk_size]

        # Sample positions along each ray
        sample_positions, deltas = generate_sample_positions(
            rays_o_chunk,
            rays_d_chunk,
            near,
            far,
            num_samples,
            device
        )

        # Flatten for batching into the network
        sample_positions_flat = sample_positions.reshape(-1, 3)
        directions_flat = (
            rays_d_chunk
            .unsqueeze(1)
            .expand(-1, num_samples, -1)
            .reshape(-1, 3)
        )

        # Query the model for colors & densities
        colors_flat, densities_flat = query_model(
            model,
            sample_positions_flat,
            directions_flat,
            near,
            far
        )
        
        # Reshape back to [batch, num_samples, ...]
        colors = colors_flat.reshape(rays_o_chunk.shape[0], num_samples, 3)
        densities = densities_flat.reshape(rays_o_chunk.shape[0], num_samples)

        # Composite final color per ray
        composed_rgb = composite_volume(colors, densities, deltas, white_background)
        rgb_out.append(composed_rgb)

    return torch.cat(rgb_out, dim=0)
